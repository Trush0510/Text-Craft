{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for reproducability\n",
    "SEED = 34\n",
    "\n",
    "#maximum number of words in output text\n",
    "MAX_LEN = 70\n",
    "input_sequence = \"In a land far away, where dragons roam freely and magic is real\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a610ed030e8a4d5bbb3562e077934e9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eeafe071c25a47ef9159b4e45eeaaccc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af659ba9736a44188ddc8ba356f2478e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb8af71730c14f3580177294d4caf6e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7369bc61cf9740d0b8487237bae21d69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/689 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ebf2f70766f4d27b82ba32b3315b516",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/6.43G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the weights of TFGPT2LMHeadModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tfgpt2lm_head_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " transformer (TFGPT2MainLay  multiple                  1557611200\n",
      " er)                                                             \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1557611200 (5.80 GB)\n",
      "Trainable params: 1557611200 (5.80 GB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#get transformers\n",
    "from transformers import TFGPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "#get GPT2 XL tokenizer and GPT2 XL model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2-xl\")\n",
    "GPT2_XL = TFGPT2LMHeadModel.from_pretrained(\"gpt2-xl\", pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "#view model parameters\n",
    "GPT2_XL.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get deep learning basics\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(SEED)\n",
    "# encode context the generation is conditioned on\n",
    "input_ids = tokenizer.encode(input_sequence, return_tensors='tf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "In a land far away, where dragons roam freely and magic is real, a young man named Riku is about to embark on a journey that will change his life forever. His only hope is to find seven magical crystals that will grant him the power to fight the evil that threatens the land. But to get there, he'll have to travel through\n",
      "\n",
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# generate text until the output length (which includes the context length) reaches 50\n",
    "greedy_output = GPT2_XL.generate(input_ids, max_length = MAX_LEN)\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(greedy_output[0], skip_special_tokens = True))\n",
    "# set return_num_sequences > 1\n",
    "beam_outputs = GPT2_XL.generate(\n",
    "    input_ids, \n",
    "    max_length = MAX_LEN, \n",
    "    num_beams = 5, \n",
    "    no_repeat_ngram_size = 2, \n",
    "    num_return_sequences = 5, \n",
    "    early_stopping = True\n",
    ")\n",
    "\n",
    "print('')\n",
    "print(\"Output:\\n\" + 100 * '-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: In a land far away, where dragons roam freely and magic is real, there is a young man who has a dream. He wants to be a hero, and he will do anything to achieve his goal. But he has no idea what he's getting himself into.\n",
      "1: \n",
      "2: \n",
      "3: \n",
      "4: \n",
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "In a land far away, where dragons roam freely and magic is real, a young woman discovers that she can wield these powers to help others. By learning to control her own magic, she sets out to explore a new world, and to learn more about herself and the world around her.\n",
      "\n",
      "You can help her do that?\n",
      "\n",
      "This\n",
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "In a land far away, where dragons roam freely and magic is real, he is a wizard of light. When he's not teaching young dragons how to use their magic, you can find him at Dragon Manor, where he lives with his beloved wife...and her pet dragons. ...\n",
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "In a land far away, where dragons roam freely and magic is real, heroes have come and gone, but there is one who, like himself, will endure. This one is Shiro. Now with the aid of a new companion, he seeks to end the endless darkness that threatens to consume his world.\n",
      "\n",
      "\n",
      "\"There is only one person ...\n"
     ]
    }
   ],
   "source": [
    "# now we have 3 output sequences\n",
    "for i, beam_output in enumerate(beam_outputs):\n",
    "      print(\"{}: {}\".format(i, tokenizer.decode(beam_output, skip_special_tokens=True)))\n",
    "# use temperature to decrease the sensitivity to low probability candidates\n",
    "sample_output = GPT2_XL.generate(\n",
    "                             input_ids, \n",
    "                             do_sample = True, \n",
    "                             max_length = MAX_LEN, \n",
    "                             top_k = 0, \n",
    "                             temperature = 0.8\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(sample_output[0], skip_special_tokens = True))\n",
    "#sample from only top_k most likely words\n",
    "sample_output = GPT2_XL.generate(\n",
    "                             input_ids, \n",
    "                             do_sample = True, \n",
    "                             max_length = MAX_LEN, \n",
    "                             top_k = 50\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(sample_output[0], skip_special_tokens = True), '...')\n",
    "#sample only from 80% most likely words\n",
    "sample_output = GPT2_XL.generate(\n",
    "                             input_ids, \n",
    "                             do_sample = True, \n",
    "                             max_length = MAX_LEN, \n",
    "                             top_p = 0.8, \n",
    "                             top_k = 0\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(sample_output[0], skip_special_tokens = True), '...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69092fea816648e39b5945a24cc2035c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"gpt2-xl\"\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Text:\n",
      "It was a bright sunny morning in London. A few days ago, I was talking to a colleague about my current work and some of the upcoming work I have been doing. He asked me what I did before I got into programming. I told him I had studied architecture at college and that I had been working on a video game engine. He asked me why I had decided to do this. I told him I was interested in the possibilities that video games could offer for children. He said that video\n"
     ]
    }
   ],
   "source": [
    "def generate_continuation(prompt_text, max_length=100, temperature=0.7, num_return_sequences=1):\n",
    "    input_ids = tokenizer.encode(prompt_text, return_tensors=\"pt\")\n",
    "    \n",
    "    # Generate continuation\n",
    "    output = model.generate(\n",
    "        input_ids,\n",
    "        do_sample=True,\n",
    "        max_length=max_length,\n",
    "        temperature=temperature,\n",
    "        num_return_sequences=num_return_sequences,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    \n",
    "    generated_texts = []\n",
    "    for sequence in output:\n",
    "        generated_texts.append(tokenizer.decode(sequence, skip_special_tokens=True))\n",
    "    \n",
    "    return generated_texts\n",
    "\n",
    "# User input\n",
    "user_prompt = input(\"Enter your statement: \")\n",
    "\n",
    "# Generate continuation\n",
    "generated_text = generate_continuation(user_prompt)\n",
    "\n",
    "# Print generated text\n",
    "print(\"\\nGenerated Text:\")\n",
    "print(generated_text[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
